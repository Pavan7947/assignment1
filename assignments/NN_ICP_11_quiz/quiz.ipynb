{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "333d3154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date      Symbol Series  Prev Close    Open     High    Low   Last  \\\n",
      "0  2007-11-27  MUNDRAPORT     EQ      440.00  770.00  1050.00  770.0  959.0   \n",
      "1  2007-11-28  MUNDRAPORT     EQ      962.90  984.00   990.00  874.0  885.0   \n",
      "2  2007-11-29  MUNDRAPORT     EQ      893.90  909.00   914.75  841.0  887.0   \n",
      "3  2007-11-30  MUNDRAPORT     EQ      884.20  890.00   958.00  890.0  929.0   \n",
      "4  2007-12-03  MUNDRAPORT     EQ      921.55  939.75   995.00  922.0  980.0   \n",
      "\n",
      "    Close    VWAP    Volume      Turnover  Trades  Deliverable Volume  \\\n",
      "0  962.90  984.72  27294366  2.687719e+15     NaN             9859619   \n",
      "1  893.90  941.38   4581338  4.312765e+14     NaN             1453278   \n",
      "2  884.20  888.09   5124121  4.550658e+14     NaN             1069678   \n",
      "3  921.55  929.17   4609762  4.283257e+14     NaN             1260913   \n",
      "4  969.30  965.65   2977470  2.875200e+14     NaN              816123   \n",
      "\n",
      "   %Deliverble  \n",
      "0       0.3612  \n",
      "1       0.3172  \n",
      "2       0.2088  \n",
      "3       0.2735  \n",
      "4       0.2741  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the stock prices dataset\n",
    "stock_data = pd.read_csv('ADANIPORTS.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(stock_data.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9d420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "# Assuming the dataset is loaded from a CSV file named 'stock_prices.csv'\n",
    "data = pd.read_csv('ADANIPORTS.csv')\n",
    "\n",
    "# Step 2: Prepare the Data\n",
    "# Extract the 'Close' prices as our target variable\n",
    "prices = data['Close'].values.reshape(-1, 1)\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_prices = scaler.fit_transform(prices)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(scaled_prices) * 0.8)\n",
    "test_size = len(scaled_prices) - train_size\n",
    "train_data, test_data = scaled_prices[0:train_size, :], scaled_prices[train_size:len(scaled_prices), :]\n",
    "\n",
    "# Function to create dataset with input features and target variable\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(dataset) - time_step - 1):\n",
    "        a = dataset[i:(i + time_step), 0]\n",
    "        X.append(a)\n",
    "        y.append(dataset[i + time_step, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Reshape into X=t and Y=t+1\n",
    "time_step = 10\n",
    "X_train, y_train = create_dataset(train_data, time_step)\n",
    "X_test, y_test = create_dataset(test_data, time_step)\n",
    "\n",
    "# Reshape input to be [samples, time steps, features] for LSTM\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Step 3: Build the Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(units=50, return_sequences=True, input_shape=(time_step, 1)))\n",
    "model.add(LSTM(units=50))\n",
    "model.add(Dense(units=1))\n",
    "\n",
    "# Step 4: Train the Model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=32)\n",
    "\n",
    "# Step 5: Evaluate the Model\n",
    "# Predictions on training and testing data\n",
    "train_predictions = model.predict(X_train)\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Inverse transform the predictions to get actual prices\n",
    "train_predictions = scaler.inverse_transform(train_predictions)\n",
    "y_train = scaler.inverse_transform([y_train])\n",
    "test_predictions = scaler.inverse_transform(test_predictions)\n",
    "y_test = scaler.inverse_transform([y_test])\n",
    "\n",
    "# Calculate metrics\n",
    "train_mae = mean_absolute_error(y_train[0], train_predictions[:, 0])\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train[0], train_predictions[:, 0]))\n",
    "test_mae = mean_absolute_error(y_test[0], test_predictions[:, 0])\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test[0], test_predictions[:, 0]))\n",
    "\n",
    "print('Training MAE:', train_mae)\n",
    "print('Training RMSE:', train_rmse)\n",
    "print('Testing MAE:', test_mae)\n",
    "print('Testing RMSE:', test_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b128bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Step 1: Load the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalize the input data\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Expand dimensions to include channel (grayscale)\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "# Step 2: Build a simple CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Step 3: Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Step 4: Train the model\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_test, y_test), callbacks=[reduce_lr])\n",
    "\n",
    "# Step 5: Evaluate the model's performance\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Step 6: Use grid search to optimize hyperparameters\n",
    "# Define hyperparameters\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'batch_size': [64, 128, 256],\n",
    "    'optimizer': ['adam', 'sgd', 'rmsprop']\n",
    "}\n",
    "\n",
    "# Initialize the model\n",
    "model = KerasClassifier(build_fn=model, epochs=10, verbose=0)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
    "grid_search_result = grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and accuracy\n",
    "print(\"Best Parameters:\", grid_search_result.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search_result.best_score_)\n",
    "\n",
    "# Step 7: Visualize training history\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
